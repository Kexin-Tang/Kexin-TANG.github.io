[{"title":"BIO、NIO和AIO","url":"/2021/05/17/BIO%E3%80%81NIO%E5%92%8CAIO/","content":"BIO同步阻塞IO。同步指的是多个IO请求会按顺序执行，阻塞指的是如果一个IO请求没有完成，就会一直等待，直到完成释放。\n\n图中fd代表 文件描述符。\nBIO的逻辑就是某个线程盯住某个特定的请求，没有请求就一直等，直到有请求并完成请求。\n\nNIONIO本身是基于事件驱动思想来完成的，其主要想解决的是BIO的大并发问题。在BIO中，会使用多线程，每个线程盯住一个资源；在NIO中使用单线程或少量多线程，多个资源复用一个线程。\n轮询\n会使用Linux的系统调用read来读取fd。仍然是使用类似循环的逻辑，不断查询i有没有请求？如果没有也不会傻等，而是换到下一个请求，所以是非阻塞的。在这种情况下，如果有n个IO，则需要调用n次系统调用。\n多路复用\n此时会使用Linux的系统调用select和read，但是此时不会调用n次系统调用。此时会先调用一次select记录下每个IO的请求与否，然后把这个信息返回。最后只调用read处理有请求的。这个过程类似于中断。\n共享内存\n上一种方法中仍然要进行文件的传输，而可以使用一个共享内存。kernel获取请求情况，然后标记在共享内存中，线程直接读取共享内存的状态即可，不用传输任何内容。\n\nAIONIO本质上还是同步，但是AIO可以实现异步。\nAIO真正实现了中断的功能，即注册一个监视资源的线程，然后OS可以去做其他事。当资源发起请求时，线程监视到这个变化，并通知OS来处理该资源的请求。\n\n总结\nAIO的做法是，每个水壶上装一个开关，当水开了以后会提醒对应的线程去处理。\nNIO的做法是，叫一个线程不停的循环观察每一个水壶，根据每个水壶当前的状态去处理。\nBIO的做法是，叫一个线程停留在一个水壶那，直到这个水壶烧开，才去处理下一个水壶。\n\n可以看出AIO是最聪明省力，NIO相对省力，BIO最愚蠢。\n","categories":["OS","IO"],"tags":["概念","OS"]},{"title":"Celery消息队列","url":"/2021/04/22/Celery%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97/","content":"什么是任务队列？任务队列就是一种类似“队列”(Queue)的数据结构，是一种用于线程或计算机之间分配工作的一种机制。可以理解为把多个任务按次序放入队列中，然后当有任务完成后就会被取出。\n什么是 Celery ？Celery是一种使用Python编写的任务队列，主要概念有：\n\n中间人（Broker）\n\n中间人（Broker）是一个消息传输的中间件。Celery通过消息机制进行通信，通常使用中间人（Broker）作为客户端和职程（Worker）调节。启动一个任务，客户端向消息队列发送一条消息，然后中间人（Broker）将消息传递给一个职程（Worker），最后由职程（Worker）进行执行中间人（Broker）分配的任务。\nCelery是没有Broker的，需要使用第三方方案。常用的有： RabbitMQ, Redis 和数据库。\n\nbackend\n\n通常程序发送的消息，发完就完了，可能都不知道对方时候接受了。为此，celery实现了一个backend，用于存储这些消息以及celery执行的一些消息和结果。一般而言，同样可以使用 Redis 或者 Django ORM 对这些结果进行存储和操作。\n\n\n\n安装 Celery本文不赘述，可以参考网上各种帖子，一般而言只需要执行以下的指令即可\npip install celery[&#x27;redis&#x27;]\n\n在 Django 中使用 Celery\n注意，在使用 Celery 前一定要启动 Redis 等服务器。\n\n配置非配置文件法app | |- tasks |    | |    |- task.py            &lt;- 用户创建，存放功能函数 |- config |    | |    |- settings.py        &lt;- Django 自带的 settings.py |    |- celeryConfig.py    &lt;- 用户创建，初始化 Celery\n\n&#x27;&#x27;&#x27;    celeryConfig.py&#x27;&#x27;&#x27;import celeryimport os# 参数0为固定设置，参数1为settings的路径os.environ.setdefault(&quot;DJANGO_SETTINGS_MODULE&quot;, &quot;config.settings&quot;)import djangodjango.setup()# main为当前模块的名称，broker设置中间人URL，backend设置存储URLapp = celery.Celery(main=&quot;videoweb&quot;, broker=&#x27;redis://localhost:6379/2&#x27;, backend=&#x27;redis://localhost:6379/3&#x27;)# 定义会使用 &#x27;app&#x27; 这个消息队列的文件的位置app.autodiscover_tasks([&#x27;app.tasks&#x27;])\n\n&#x27;&#x27;&#x27;    task.py&#x27;&#x27;&#x27;from app.config.celeryConfig import appimport time@app.taskdef func():    # 一些需要放在异步队列中运行的，比较耗时/会造成阻塞的任务代码\n\n配置文件法app | |- tasks |    | |    |- task.py            &lt;- 用户创建，存放功能函数 |- config |    | |    |- settings.py        &lt;- Django 自带的 settings.py |- celery |    | |    |- celeryConfig.py    &lt;- 用户创建，设置信息 |    |- celeryApp.py       &lt;- 用户创建，初始化 celery\n\n\n注意， celery 相关的设置和初始化需要放在同一个文件夹下\n\n&#x27;&#x27;&#x27;    celeryConfig.py&#x27;&#x27;&#x27;BROKER_URL = &#x27;redis://127.0.0.1:6379&#x27;               # 指定 BrokerCELERY_RESULT_BACKEND = &#x27;redis://127.0.0.1:6379/0&#x27;  # 指定 Backend                        CELERY_IMPORTS = (                                  # 指定导入的任务模块    &#x27;app.tasks.task&#x27;)\n\n&#x27;&#x27;&#x27;    celeryApp.py&#x27;&#x27;&#x27;from celery import Celeryapp = Celery(&#x27;videoweb&#x27;)app.config_from_object(&#x27;celeryConfig&#x27;)   # 加载配置\n\n&#x27;&#x27;&#x27;    task.py&#x27;&#x27;&#x27;from app.celery.celeryApp import app@app.taskdef func():    # 一些需要放在异步队列中运行的，比较耗时/会造成阻塞的任务代码\n\n使用在配置完成后，需要手动启动 celery 服务\n# 代表将使用videoweb注册的app放入异步队列，使用4个workercelery -A videoweb worker -c 4 -l info  \n\n在其他文件中需要调用func()时，可以使用func.delay()的格式，即可将func()函数放到异步队列中运行。\n\n注意，在使用.delay(params)的时候，params只能是可解析格式，如list, str, int, json等，不能是类似于Queryset之类的内容。\n\n查看执行的结果由于是异步进程，执行的结果可能不会在编写的代码中体现，所以需要在后续重新读取执行的结果并对页面等进行相应的改动。\n实现该过程的方法就是使用 AJAX。这并不在本文的讨论范围内，本文主要告知如何访问消息队列中的程序。\n在使用func.delay()时，会返回一个 AsyncResult 实例，其有一些属性和方法，如 id, successful() 和 *failed()*。一般而言，我们会将 id 一并传入 Django 的 Template 视图中，然后编写 AJAX，读取 id 对应线程的执行结果，并根据是否成功处理页面。\n&#x27;&#x27;&#x27;    views.py&#x27;&#x27;&#x27;def get(req):    ...    task = func.delay(params)    id = task.id    data[&#x27;id&#x27;] = id        ...    return render_to_redirect(req, self.TEMPLATE, data=data)\n\n详细的使用 Celery + AJAX 的操作我也不会日后探究 /斜眼笑。\n","categories":["Python","Web","Django","Python"],"tags":["python","django","消息队列","redis"]},{"title":"HTTP和HTTPS","url":"/2021/04/26/HTTP%E5%92%8CHTTPS/","content":"HTTP超文本传输协议，是一个基于请求与响应，无状态的，应用层的协议，常基于TCP/IP协议传输数据，互联网上应用最为广泛的一种网络协议,所有的WWW文件都必须遵守这个标准。设计HTTP的初衷是为了提供一种发布和接收HTML页面的方法。\nHTTP主要有三个版本：HTTP1.0，HTTP1.1，HTTP2.0：\n\n在HTTP1.1中规定，一个TCP连接中可以传输多个HTTP请求，且支持了持久化连接，不会发送一个HTTP请求就断开\n在HTTP2.0中规定，一个TCP连接中可以将多个HTTP请求合在一起发送/接收，这是2.0的新特性\n\nHTTP的特点\n无状态：协议对客户端没有状态存储，对事物处理没有“记忆”能力，比如访问一个网站需要反复进行登录操作\n无连接：HTTP/1.1之前，由于无状态特点，每次请求需要通过TCP三次握手四次挥手，和服务器重新建立连接。比如某个客户机在短时间多次请求同一个资源，服务器并不能区别是否已经响应过用户的请求，所以每次需要重新响应请求，需要耗费不必要的时间和流量。\n基于请求和响应：基本的特性，由客户端发起请求，服务端响应\n简单快速、灵活\n通信使用明文、请求和响应不会对通信方进行确认、无法保护数据的完整性\n\n\nHTTPS可以视为加入了SSL的HTTP。HTTPS经由HTTP进行通信，利用SSL加密数据包。HTTPS使用的主要目的是提供对网站服务器的身份认证，同时保护交换数据的隐私与完整性。\n在加密的过程中，HTTPS使用了对称加密和非对称加密相结合的方法。\n对称加密 Vs 非对称加密对称加密即加密和解密的方法一致，比如客户端加密文件，用了一个密码123456(或者是通过了某个加密算法后)，那么服务端只要输入123456(或通过了客户端加密算法对应的解密算法后)即可以实现解密。\n非对称加密分为公钥和私钥，其中私钥是服务器独有，公钥是公开发送给客户端。公钥可以加密私钥，私钥也可以解密公钥，但是公钥无法解密公钥。\n中间人攻击在Client和Server通信的时候，理论上的流程如下：\n\nClient向Server发送请求，包括客户端浏览器支持的加密算法等信息\nServer发现是第一次请求，所以返回数据并附带自己保存的私钥0 对应的 公钥0\nClient收到 公钥0，之后每次发数据都用 公钥0 进行加密\nServer收到 公钥0 加密信息后使用 私钥0 解密即可\n\n但是在过程中，可能有中间人使坏，使流程变成如下：\n\nClient向Server发送请求，包括客户端浏览器支持的加密算法等信息\nServer发现是第一次请求，所以返回数据并附带自己保存的私钥0 对应的 公钥0\n中间人截获 公钥0，自己创建一个 私钥1，并发送 公钥1 给Client\nClient之后使用 公钥1 加密信息\n每次信息都被中间人截获，使用 私钥1 解码Client，然后把篡改的信息用 公钥0 加密发给Server\n\n\n\n\nHTTPS加密的过程CA证书证书是一种由大家公认的第三方机构颁发的一种证明，用于验证服务器是安全可靠的。一般公司需要向第三方机构提出申请，然后第三方机构验证合法性后进行颁发。\n比如网站www.lolol.com想要申请证书，第三方机构可能会要求我向服务器中放入一个指定的文件，或者是DNS重定向某个随机网页到www.lolol.com来证明我对服务器能够拥有权限。\n数字签名数字签名用于验证是否是某个私钥给我发送的信息。\n数字签名和加密解密过程类似，不过是反过来的：加密的是公钥，解密的是私钥；签名的是私钥，验证的是公钥。\n数字签名的特点：\n\n认证：确认给我发送签名的是拥有私钥的\n防抵赖：发送方一旦发送，不能反悔\n防篡改：数字签名的生成是和文件内容有关的，如果更改内容必然更改了数字签名，也就无法再解密\n\nHTTPS加密过程","categories":["Web","Concept"],"tags":["web","http","concept"]},{"title":"Django中Cookie和Session实现登录/注销","url":"/2021/04/25/Django%E4%B8%ADCookie%E5%92%8CSession%E5%AE%9E%E7%8E%B0%E7%99%BB%E5%BD%95-%E6%B3%A8%E9%94%80/","content":"什么是 cookies &amp; session?会话（Session）跟踪是Web程序中常用的技术，用来跟踪用户的整个会话。常用的会话跟踪技术是Cookie与Session。Cookie通过在客户端记录信息确定用户身份，Session通过在服务器记录信息确定用户身份\ncookies在程序中，会话跟踪是很重要的事情。理论上，一个用户的所有请求操作都应该属于同一个会话，而另一个用户的所有请求操作则应该属于另一个会话，二者不能混淆。\n而Web应用程序是使用HTTP协议传输数据的。HTTP协议是无状态的协议。一旦数据交换完毕，客户端与服务器端的连接就会关闭，再次交换数据需要建立新的连接。这就意味着服务器无法从连接上跟踪会话。\nCookie就是这样的一种机制。它可以弥补HTTP协议无状态的不足。即每次HTTP协议只需要访问到同一个Cookie则认为请求和相应是同一个对象接收和发出。\nsession客户端浏览器访问服务器的时候，服务器把客户端信息以某种形式记录在服务器上。这就是Session。客户端浏览器再次访问时只需要从该Session中查找该客户的状态就可以了。\n\n比较常见的就是登陆。如果用户在客户端通过登陆验证，那么就会设置一个session来标记某个用户已经登录，访问别的网页时可以通过 middleware 访问session来查看某个用户是否已经登录/登录是否过期。\n\n两者的比较一个存储在客户端，一个存储在服务端。如果把访问的过程看做一个人参观某个公司，那么：\n\n如果不使用cookie和session，那么他每次进出都需要找保安报备\ncookies相当于给这个人配一个临时卡(身份识别保存在客户端)，他刷卡就可以进入公司，参观结束后(cookies生命周期结束后)则回收卡(消除各种数据)\nsession相当于公司的门禁处设置人脸识别(身份识别在服务端)，他通过验证即可，参观结束后消除人脸识别机器的相关信息(服务器进行flush)\n\n在Django实现cookies和session登录/注销Cookies 版本验证是否登录的装饰器&#x27;&#x27;&#x27;    permission.py&#x27;&#x27;&#x27;def checkLoginByCookies(func):    @functools.wrap(func)    def wrapper(self, request, *args, **kwargs):        # COOKIES_NAME是保存cookies的key名字        # 可以自定义，只要保证在设置cookies的时候也是用了这个名字即可        id = request.COOKIES.get(COOKIES_NAME)                # 如果没有cookies信息，则保存当前路径，并跳转到登录界面        # 这样在登录后可以直接跳回当前路径        if not id:            return redirect(&#x27;&#123;&#125;?to=&#123;&#125;&#x27;.format(reverse(&#x27;DESTINATION&#x27;), request.path))          # 如果这样定义，则说明设置cookies的时候保存的为&#123;COOKIES_NAME: user.id&#125;        user = User.objects.filter(pk=id)        if not user:            return redirect(&#x27;&#123;&#125;?to=&#123;&#125;&#x27;.format(reverse(&#x27;DESTINATION&#x27;), request.path))            # 如果一切正常，则执行封装的函数        return func(self, request, *args, **kwargs)              return wrapper\n\n登录界面class Login(View):    TEMPLATE = &#x27;login.html&#x27;    def get(self, req):        id = req.COOKIES.get(COOKIES_NAME)        # 如果在已经登录的情况下访问login界面，则自动跳回主页        if id:            return redirect(reverse(&#x27;HOMEPAGE&#x27;))        # 如果还未登录，则需要登录，登录完成后跳回当前的路径        to = req.GET.get(&#x27;to&#x27;, &#x27;&#x27;)        return render_to_response(req, self.TEMPLATE, &#123;&#x27;to&#x27;: to&#125;)    &#x27;&#x27;&#x27;        此函数中调用了JsonResponse是用于AJAX显示错误提示    &#x27;&#x27;&#x27;    def post(self, req):        username = req.POST.get(&#x27;username&#x27;)        password = req.POST.get(&#x27;password&#x27;)        # 查看是否存在用户        exists = User.objects.filter(username=username).exists()        if not exists:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &#x27;该用户不存在&#x27;&#125;)        # 查看用户账号密码是否匹配        user = authenticate(username=username, password=password)        if not user:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &quot;登录失败&quot;&#125;)        user = User.objects.get(username=username)        if user.is_staff is False:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &quot;缺少权限，请联系管理员&quot;&#125;)        current_time = datetime.datetime.utcnow()                   # 获取当前时间        expire_time = current_time + datetime.timedelta(minutes=5)  # 推迟五分钟作为过期时间        # 配置cookie        response = JsonResponse(&#123;&#x27;status&#x27;: 0, &#x27;error&#x27;: &#x27;&#x27;&#125;)        response.set_cookie(COOKIES_NAME, str(user.id), expires=expire_time)        return response\n\n注销class Logout(View):    def get(self, req):        response = redirect(reverse(&#x27;LOGIN&#x27;))        response.delete_cookie(COOKIES_NAME)  # cookies版本        return response\n\nSession 版本验证是否登录的装饰器def checkLoginBySession(func):    @functools.wraps(func)    def wrapper(self, request, *args, **kwargs):        # is_login和SESSION_NAME都是用户自定义key名字        status = request.session.get(&#x27;is_login&#x27;)        id = request.session.get(SESSION_NAME)        # 如果状态为未登录        if status != &#x27;true&#x27;:            return redirect(&#x27;&#123;&#125;?to=&#123;&#125;&#x27;.format(reverse(&#x27;LOGIN&#x27;), request.path))        user = User.objects.filter(pk=id).exists()        if user:            return func(self, request, *args, **kwargs)        else:            return redirect(&#x27;&#123;&#125;?to=&#123;&#125;&#x27;.format(reverse(&#x27;LOGIN&#x27;), request.path))    return wrapper\n\n\n登录class Login(View):    TEMPLATE = &#x27;/login.html&#x27;    def get(self, req):        id = req.session.get(SESSION_NAME)        if id:            return redirect(reverse(&#x27;HOMEPAGE&#x27;))        to = req.GET.get(&#x27;to&#x27;, &#x27;&#x27;)        return render_to_response(req, self.TEMPLATE, &#123;&#x27;to&#x27;: to&#125;)    def post(self, req):        username = req.POST.get(&#x27;username&#x27;)        password = req.POST.get(&#x27;password&#x27;)        # 查看是否存在用户        exists = User.objects.filter(username=username).exists()        if not exists:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &#x27;该用户不存在&#x27;&#125;)        # 查看用户账号密码是否匹配        user = authenticate(username=username, password=password)        if not user:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &quot;登录失败&quot;&#125;)        user = User.objects.get(username=username)        if user.is_staff is False:            return JsonResponse(&#123;&#x27;status&#x27;: -1, &#x27;error&#x27;: &quot;缺少权限，请联系管理员&quot;&#125;)        # 配置session        req.session[SESSION_NAME] = user.id        req.session[&#x27;user_name&#x27;] = user.username        req.session[&quot;is_login&quot;] = &quot;true&quot;        req.session.set_expiry(300)         # 300s后失效        response = JsonResponse(&#123;&#x27;status&#x27;: 0, &#x27;error&#x27;: &#x27;&#x27;&#125;)        return response\n\n注销class Logout(View):    def get(self, req):        response = redirect(reverse(&#x27;LOGIN&#x27;))        req.session.flush() # session版本        return response","categories":["Python","Web","Django","Python"],"tags":["python","django"]},{"title":"Celery和AJAX配合实现异步刷新","url":"/2021/04/23/Celery%E5%92%8CAJAX%E9%85%8D%E5%90%88%E5%AE%9E%E7%8E%B0%E5%BC%82%E6%AD%A5%E5%88%B7%E6%96%B0/","content":"需求介绍由于 Celery 是异步队列，有时候用户的页面已经显示出来，但是后台仍在执行任务，我们想实现这样一个功能：当后台执行任务完成后，前端页面相应进行改变。\n这样的场景和需求非常常见，比如上传视频这个过程非常耗时，我们希望用户点击上传视频后，页面马上显示一个区域，表示某视频正在上传（比如视频的图片、标题、简介等马上显示出来），但是此时还不能点击观看（disable链接），只有当后台上传完毕后，才能在前端开放（enable链接）。\n在之前的Celery消息队列中已经介绍过 Celery，此处先简介 Ajax。\nAJAXAsynchronous JavaScript and XML。可以实现每次只使用HTTP请求一小部分资源并加载页面的某一些部分，而不用请求整个页面的所有资源并刷新整个页面所有内容。比较常见的 AJAX 使用场景就是在网页中“下拉显示更多”这样的操作。\n一般而言使用 AJAX 都需要搭配使用 jQuery，其常见的语法如下：\n$(document).ready(function()&#123;            $(&quot;.btn&quot;).click(function() &#123;        // 点击btn时会调用下面的函数                let id = $(&#x27;#id&#x27;).val();        // 获取id的值                $.ajax(&#123;                    url: &quot;&quot;,                    // 要发送请求的url，可以使用mako等模板                    type: &quot;GET&quot;,                // 请求的方式                    data: &#123;&#x27;id&#x27;: id&#125;,           // 一并发送的数据                        success: function(results)&#123; // 如果成功请求                        ...                    &#125;,                    error: function(error) &#123;    // 如果发生错误                        ...                    &#125;                &#125;);            &#125;);        &#125;);\n\n代码\n首先将操作放入 celery 异步队列中，得到一个 &lt;celery.result.AsyncResult&gt; 返回，最重要的是访问其中 id 属性，可以通过 id 访问最终队列的执行结果；\n&#x27;&#x27;&#x27;    function.py&#x27;&#x27;&#x27;class Func(View):    def get(req):        ...        task = celeryFunction.delay() # 得到一个&lt;celery.result.AsyncResult&gt;        ...        return render_to_redirect(req, self.TEMPLATE, data = &#123;&#x27;task_id&#x27;: task_id&#125;)  # 一定要将 task_id 保存住\n然后编写HTML页面用于直接显示内容，可以包含已处理的和待处理的，里面加上 ajax，而 ajax 中的 url 指向某个路由，用于访问执行结果并进行返回；\n$(document).ready(function()&#123;            $(&quot;.refresh&quot;).click(function() &#123;                let task_id = $(&#x27;#task_id&#x27;).val();                $.ajax(&#123;                    url: &quot;$&#123;reverse(&#x27;check&#x27;)&#125;&quot;,                    type: &quot;GET&quot;,                    data: &#123;&#x27;task_id&#x27;: task_id&#125;,                    success: function(results)&#123;                        // 根据最终返回的JSON内容改变本页面的内容                    &#125;,                &#125;);            &#125;);        &#125;);\n在 urls.py 中将上面 ajax 需要跳转的页面进行注册\nurlpatterns &#x3D; [    ...    path(&#39;check&#39;, views.check, name&#x3D;&quot;check&quot;),    ...]\n编写上述新加入的页面函数逻辑，并返回 JsonResponse 进入 ajax 的处理函数中（如 success 中，从而可以改变原有页面内容）\n&#x27;&#x27;&#x27;    views.py&#x27;&#x27;&#x27;from django.http import JsonResponsefrom celery.result import AsyncResultdef check(request):    task_id = request.GET.get(&#x27;task_id&#x27;)    if task_id:        task_res = AsyncResult(task_id)        return JsonResponse(&#123;&#x27;finish&#x27;: task_res.ready()&#125;)    return JsonResponse(&#123;&#x27;finish&#x27;: False&#125;)\n\n坑\n在返回给 ajax 信息时，一定要使用类似于 JSON 的格式，所以推荐使用 JsonResponse。\n\n如果你的 Celery 不幸给你返回了 HTTP 500，那么恭喜你，你应该是没有在 Celery 里设置 backend，也就是说你的服务器只负责帮你执行任务，但是不会把结果及相关信息存储到数据库。（别问我怎么知道的，现在正在东九五楼吹风呢）\n\n\n","categories":["Python","Web","Django","Python"],"tags":["python","django","消息队列","redis"]},{"title":"How to write 'hello world' in HEXO (I)","url":"/2021/04/18/How-to-write-hello-world-in-HEXO-I/","content":"What is HEXO?HEXO is a very famous and useful web framework. People always combine HEXO with GitHub to bulid their own blogs or webpages.\nInstallFirst of all, you need to install node.js, you can refer to its website here, and git, you can refer here.\nAfter configuration, you are supposed to install HEXO via node.js command called npm.\nnpm install -g hexo-cli\n\nInitializeOnce HEXO is installed, run the following commands to initialize HEXO in the target &lt;folder&gt;.\nhexo init &lt;folder&gt;  # initcd &lt;folder&gt;npm install         # install other dependency\n\nOnce initialized, here’s what your project folder will look like:\n.├── _config.yml         # user can config settings here├── package.json        # record dependency etc.├── scaffolds           # define three post methods -- post, draft and page├── source              # store your posts and other resources|   ├── _drafts|   └── _posts└── themes              # user&#39;s custom themes\n\nConfigurationIn _config.yml file, you can stylize your own settings. For example:\n\ntitle – your website’s name\nurl – the URL of your website\n\nyou can find more details here.\nCommandnewhexo new [layout] [-p/--path filePath] &lt;title&gt;\n\nCreates a new article. If no [layout] is provided, Hexo will use the default_layout from _config.yml. \nBy default, Hexo will use the title to define the path of the file. For page, it will create a directory of that name and an index.md file in it. Use the –path option to override that behaviour and define the file path:\nhexo new page --path about/me &quot;About&quot;\nwill create source/about/me.md file with the title “About” set in the front matter.\nFor post, it will automatically generate a new file in source/_posts/&lt;title&gt;.md\nserverhexo serverhexo s\nRun a local server.\ngeneratehexo g\nGenerate static files, which means it will convert/render .md or .ejs files to html files.\ncleanhexo clean\n\nCleans the cache file (db.json) and generated files (public).\ndeployhexo d\n\nDeploy your files to remote server. You can set your deploy destination:\n\nopen _config.yml\nconfigure depoly area, for example, if you want to deploy your website in GitHubdeploy:    type: git    repository: https://github.com/xxx/xxx.github.io.git    branch: master\n\n","categories":["Web","HEXO","Framework","HEXO"],"tags":["web","hexo","framework"]},{"title":"Python4Redis","url":"/2021/05/12/Python4Redis/","content":"本文主要介绍基于Python来连接、操作Redis数据库。在Python中，使用redispy库来进行连接。关于redispy详细的文档可以参考GitHub。\n连接Redisimport redis# 简历连接实例，参数为&quot;主机&quot;，&quot;端口&quot;，&quot;数据库id&quot;r = redis.Redis(host=&#x27;localhost&#x27;, port=6379, db=0, decode_responses=True)\n\n在Python3中返回的是bytes类型，想要直接获取str类型，需要设置decode_responses=True\n\n注意，在redispy中是没有设置SELECT操作更换数据库id，所以请为每个数据库都创建独立的连接实例。\n在默认情况下，redispy会自动给每个连接实例创建对应的连接池，但是用户也可以手动将连接池实例放入已存在的连接池中。\npool = redis.ConnectionPool(host=&#x27;localhost&#x27;, port=6379, db=0)r = redis.Redis(connection_pool=pool)\n\n\nPipleline在Redis中为了节省RTT，会使用pipeline将多个指令一并发送。在python中的操作如下：\npipe = r.pipeline()pipe.operation0()pipe.operation1()...pipe.operationN()pipe.execute()\n\n由于pipeline是放入缓冲区顺序执行的，也可以在代码中串联起来\npipe.operation0().operation1(). ... .operationN().execute()\n\n在redispy中，上述pipeline过程会自动被转换成“事务”，以保证原子性。如果不希望自动被转换成“事务”，则需要设置pipe = r.pipeline(transaction=False)。\n\nTransaction在redispy中，可以使用pipeline实现transaction\npipe = r.pipeline()while True:    try:        pipe.watch(&#x27;&lt;key&gt;&#x27;) # 加锁        pipe.multi()        # 组队            ...        pipe.execute()      # 执行        break    except WatchError:        continue    finally:        pipe.reset()        # 归还到连接池，默认情况下会自动调用\n\n存在一种更便捷的方法，用于处理try-catch和加锁等操作。它需要一个回调函数，一个管道对象和任意数量的键都将被监视\ndef transactionFunc(pipe):    pipe.multi()    ...    pipe.execute()# 传入回调函数和需要被加锁监视的keyr.trascation(transactionFunc, &lt;watched_key&gt;)\n\n\n数据类型对于基本的Redis操作都有支持。\n\n对于最普通的，传入参数都是func(key: str, val: str/list/tuple)\n对于多个设置，如mset等，传入的都是键值对的mapping\n对于设置时间相关的，如setex等，func(key: str, time: int, value: str)\n对于有范围的，如lrange等，传入的都是func(key: str, start: int, end: int)\n对于有个数的，如lrem等，传入的参数func(key: str, num: int, value: str)\n对于有间隔的，如incrby等，传入的参数func(key: str, step: int)\n\n\n注意，对于list/tuple等，传入时参数要为*args；对于dict，传入时参数要为**kwargs\nvalues = [&#x27;bob&#x27;, &#x27;jack&#x27;]r.lpush(&#x27;user&#x27;, *values)\n\n\nPubSub订阅订阅非常简单，只需要输入以下的指令创建pubsub实例，然后订阅指定的频道即可，其中psubscribe可以匹配正则表达式。\nr = redis.Redis(...)p = r.pubsub()p.subscribe(&#x27;channel0&#x27;, &#x27;channel1&#x27;, ...)p.psubscribe(&#x27;channel*&#x27;)    # 订阅任何channel开头的频道\n\n通过get_message可以获得实例对应的信息。其中消息体的具体含义：\n\ntype – subscribe, unsubscribe, psubscribe, punsubscribe, message, pmessage中的某一个\nchannel – 订阅的频道或者是有新消息的频道\npattern – 与已发布消息的频道匹配的模式。 除pmessage类型外，在所有情况下均为None\ndata – 消息信息。如果是有消息，则为消息内容，否则为频道id\n\n&gt;&gt;&gt; p.get_message()&#123;&#x27;pattern&#x27;: None, &#x27;type&#x27;: &#x27;subscribe&#x27;, &#x27;channel&#x27;: b&#x27;channel1&#x27;, &#x27;data&#x27;: 1&#125;&gt;&gt;&gt; p.get_message()&#123;&#x27;pattern&#x27;: None, &#x27;type&#x27;: &#x27;subscribe&#x27;, &#x27;channel&#x27;: b&#x27;channel0&#x27;, &#x27;data&#x27;: 2&#125;&gt;&gt;&gt; p.get_message()&#123;&#x27;pattern&#x27;: None, &#x27;type&#x27;: &#x27;psubscribe&#x27;, &#x27;channel&#x27;: b&#x27;channel*&#x27;, &#x27;data&#x27;: 3&#125;\n\n发布使用p.publish进行消息发布，可以指定发布的位置和消息内容。\n# the publish method returns the number matching channel and pattern# subscriptions. &#39;my-first-channel&#39; matches both the &#39;my-first-channel&#39;# subscription and the &#39;my-*&#39; pattern subscription, so this message will# be delivered to 2 channels&#x2F;patterns&gt;&gt;&gt; r.publish(&#39;my-first-channel&#39;, &#39;some data&#39;)2&gt;&gt;&gt; p.get_message()&#123;&#39;channel&#39;: b&#39;my-first-channel&#39;, &#39;data&#39;: b&#39;some data&#39;, &#39;pattern&#39;: None, &#39;type&#39;: &#39;message&#39;&#125;&gt;&gt;&gt; p.get_message()&#123;&#39;channel&#39;: b&#39;my-first-channel&#39;, &#39;data&#39;: b&#39;some data&#39;, &#39;pattern&#39;: b&#39;my-*&#39;, &#39;type&#39;: &#39;pmessage&#39;&#125;\n\n取消订阅对于subscribe，使用unsubscribe；对于psubscribe，使用punsubscribe。其可以指定取消订阅的频道名字/模式串，如果不传入任何参数则视为对所有频道取消订阅。\np.unsubscribe()p.punsubscribe(&#x27;channel*&#x27;)\n\n通过回调函数redis-py支持使用回调函数实现pubsub。回调函数要传入一个message变量，该变量为dict类型，就类似于之前的get_message返回的dict。如果想要订阅一个频道，则需要传入一个dict，其key为channelName，value为回调函数。\n当一个信息被读取后，handler会自动帮我们在message这个dict中保存信息，并调用回调函数，所以get_message返回的都是None而非信息。\ndef messageHandler(message):    print(message)p.subcribe(**&#123;&#x27;mychannel&#x27;: messageHandler&#125;) # 订阅mychannelp.publish(&#x27;mychannel&#x27;, &#x27;hello redis&#x27;)   # 此时message中有&#123;data: &#x27;hello redis&#x27;&#125;msg = p.get_message(&#x27;mychannel&#x27;)    # 打印message，但是返回给msg的为Noneprint(msg)  # None\n\n只接收新消息通过设置p = r.pubsub(ignore_subscribe_messages=True)，这样在p.get_message()时，如果没有新消息，则不会返回任何值；如果有信息消息，才会返回值。\n读取消息轮询get_message内部的实现机制是快速的调用select，在多个数据库之间遍历，寻找是否有新的消息，如果有则获取，否则返回None。\nwhile True:    message = p.get_message()    if message:        ...        time.sleep(0.001)  # be nice to the system :)\n\n监听listen()是一个生成器，其会阻塞其他进程直到有消息。\nfor message in p.listen():    ...\n\n另起线程可以将事件的循环放入另一个独立的线程中。run_in_thread()可以创建一个独立的线程，并在该线程内部开启一个循环，不断使用第一种方法查看是否有可用的信息。\n注意：要使用该方法，必须使用回调函数。\np.subscribe(**&#123;&#x27;my-channel&#x27;: my_handler&#125;)thread = p.run_in_thread(sleep_time=0.001)# the event loop is now running in the background processing messages# when it&#x27;s time to shut it down...thread.stop()\n\n\n使用Lua脚本redis-py支持 EVAL, EVALSHA, 和 SCRIPT 这些操作，但是为了让代码更具有Python的风格，同时省去一些不必要的麻烦，我们可以使用独有的方法来加载并执行Lua脚本。\nlua_script = r.register_script(lua) # 传入lua代码，返回脚本实例lua_script(keys=[...], args=[...])  # 执行脚本，其中keys传入参数作为LUA中的KEYS，args作为ARGV\n\n在执行lua_script时，还可以设置client，用于表明脚本在哪个平台上运行。如果不指明，则默认为注册lua_script的平台。\n","categories":["Redis"],"tags":["redis"]},{"title":"How-to-write-hello-world-in-HEXO-II","url":"/2021/04/19/How-to-write-hello-world-in-HEXO-II/","content":"Post your fileNow, you can begin to write your markdown or ejs files. When you want to post a file, you can just run codes in orders.\n\nhexo new post &lt;fileName&gt;\nwrite something\nhexo clean\nhexo g\nhexo d\n\nFront-matterWhen you new a file, HEXO will automatically generate a file in certain format, it depends on your METHOD (like post, draft and page) in scaffolds.\nFor POST METHOD, it will generate a file with beginning below:\n---title: &lt;fileName&gt;date: &lt;createTime&gt;---\n\nIn HEXO, the beginning area which surrounded by --- is HEXO’s syntax. title represents the file’s title, and data represents the create time. \nFor instance, when we use below Front-matter, we can get corresponding webpage.\n---title: HELLOdate: 2021-04-19---&lt;main&gt;\n\n&lt;h1&gt;HELLO&lt;&#x2F;h1&gt;[Username] &lt;span&gt;2021-04-19&lt;&#x2F;span&gt; &lt;main&gt;\n\n\ncategories &amp; tagsExcept title and date, there are many other key words:\n\ncategories – indicates the categories it belongs to\ntags – indicates the tags it has\n\nNotice: categories is different with tags. tags is unordered, while categories is ordered.\n---title: date:categories:- Web- HEXOtags:- web- hexo---\n\nThis file will be attached tags with “web” and “hexo”, these tags are parallel. However, it also will be archived to Web and HEXO, and Web is father category, HEXO is child category.\n\n\nIf you want to assign many father-child categories, you can use []\ncategories:- [Web, HEXO]- [Web, Framework]\n","categories":["Web","HEXO","Framework","HEXO"],"tags":["web","hexo","framework"]},{"title":"Python中的浅拷贝与深拷贝","url":"/2021/05/20/Python%E4%B8%AD%E7%9A%84%E6%B5%85%E6%8B%B7%E8%B4%9D%E4%B8%8E%E6%B7%B1%E6%8B%B7%E8%B4%9D/","content":"可变对象 VS 不可变对象在Python中，诸如int, string, tuple等称为不可变对象，主要是因为他们不能直接对数据进行修改，比如string中不能直接更改某一个位置的字符，tuple不能直接更改某一个内容。\na = &quot;hello&quot;# 不能直接改变内容，如操作 a[1] = &#x27;w&#x27; 是不允许的print(id(a))    # 1008a = &quot;world&quot;     # 这样的操作意味着内存中新建一个&quot;world&quot;，地址与&quot;hello&quot;不同，然后让a这个引用指向新的内存地址print(id(a))    # 1010\n\na = 1print(id(a))    # 1008a += 1          # 相当于计算出结果为2，然后内存中新建2，地址与1不同，然后a引用指向新的地址print(id(a))    # 1010\n\n诸如list, dict, set等称为可变对象，主要是他们可以直接对数据进行修改，如list可以通过索引访问并修改内容。\na = [1,2,3]print(id(a))    # 1008a.pop()print(id(a))    # 1008，即内存位置没有改变\n\n\nis VS ==a == b是用于判断数值是否相等；a is b是用于判断在内存中是否存储在同一个位置，即是否为同一个对象，也就是id(a) == id(b)\n\n引用由于Python的逻辑是“万物皆对象”，所以每个变量名都是一个引用。具体而言，比如定义a = 1，那么在内存中有一个int 1，然后a作为引用指向该地址。同理，定义b = [1,2,3]，那么内存中有一片区域[1,2,3]，然后b作为引用指向该地址。\n不可变对象对于不可变对象而言，同样的内容在内存中只会存储一个。\n由于相同内容内存中只有一个，所以不存在拷贝的问题。\n\n拷贝：内容一样，但是存储的地点不同。\n\na = 1aa = aprint(aa is a)  # True，因为`1`只在内存中存储了一个，aa和a都是指向内存同一个地方aa = 2          # 相当于在内存中新建一个`2`，然后设置一个引用名为aa，指向该内存地址print(a, aa, aa is a)   # 1, 2, False\n\na = 1   # 1008 -&gt; 1b = 2   # 1010 -&gt; 2a += 1  # 此时结果为2，由于内存中已经有了2，所以直接新建一个引用a，指向已有的内存地址1010print(a is b)   # True\n\n可变对象对于可变对象而言，同样的内容可能存储在不同的内存区域。因此，只有可变对象才需要讨论浅拷贝和深拷贝。\na = [1,2,3] # 1008b = [1,2,3] # 1010print(a is b)   # False，因为a和b虽然内容一样，但是指向两个不同的对象，这两个对象在内存的地址也不同\n\n\n引用 VS 浅拷贝 VS 深拷贝import copya = [1,2]b = [3,4]c = [a,b, 100]d = c                   # 引用e = copy.copy(c)        # 浅拷贝f = copy.deepcopy(c)    # 深拷贝\n\n\n引用引用就是直接赋值，相当于只是起了一个别名，指向的位置没有任何变化，原数据的任何变动都会影响该引用。\n浅拷贝使用copy.copy()或者是[:]都是浅拷贝，意思就是直接拷贝原引用的指向，并不拷贝指向的内容。\n深拷贝使用copy.deepcopy()，意思就是拷贝指向和指向的内容，新建一个完全独立的内容，不会受任何影响。\n对于不可变元素内包含可变元素时对于不可变元素，如果内部存储的仍然是不可变元素，那么就不存在拷贝的概念。\n如果不可变元素内存储了任意一个可变元素，那么copy和引用是一样的，但是deepcopy仍然会执行。\na = [1,2]b = [3,4]c = (a, b)    # 引用和copy地址一样，deepcopy会创建独立的d = (1, 2)    # 引用、copy和deepcopy地址一样，无差别","categories":["Programming Language","Python"],"tags":["概念","python"]},{"title":"Python中的特殊成员与魔法方法","url":"/2021/05/20/Python%E4%B8%AD%E7%9A%84%E7%89%B9%E6%AE%8A%E6%88%90%E5%91%98%E4%B8%8E%E9%AD%94%E6%B3%95%E6%96%B9%E6%B3%95/","content":"特殊成员和魔法方法是一系列特定结构的成员/方法，其格式为__XXX__，使用这些成员和方法通常会有比较奇特的效果，比如说 使用某个函数时就会自动调用某个方法，想要查看隐藏的信息就要访问特殊的成员 等。\n特殊成员__class__用于查看类名\nclass MyClass:    ...myclass = MyClass()print(myclass.__class__)    # __main__.MyClass\n\n__module__用于查看类的来源\nfrom other.modules import MyClassmyclass = MyClass()print(myclass.__module__)    # other.modules\n\n__dict__用于查看属性\nclass MyClass:    def __init__(self, name, age):        self.name = name        self.age = agemyclass = MyClass(&#x27;tom&#x27;, 18)print(myclass.__dict__) # &#123;name: &#x27;tom&#x27;, age: 18&#125;\n\n__str__定义打印输出时的显示内容\nclass MyClass:    def __str__(self):        return &quot;hello world&quot;myclass = MyClass()print(myclass)  # hello worldsentence = &quot;I want to say: %s&quot; % myclassprint(sentence) # I want to say: hello world\n\n\n魔法方法__new__(cls, ...) 和 __init__(self, ...)注意，__new__(cls, ...)为构造函数，用于创建类对象，而__init__(self, ...)仅仅是初始化函数，用于将预设的内容传递到新建的类内。这两个函数在使用诸如myClass = MyClass()时自动调用。\n__call__(self, *args, **kwargs)使得类和函数一样，可以直接加上()并传入参数调用。\nclass Add:    def __call__(self, *args):        return sum(args)myadd = Add()myadd(1,2,3)    # 6\n\n__getitem__(self, key) 和 __setitem__(self, key, value)使得对象可以使用[]进行获取和设置，就类似于字典一样。\nmyclass[key]            # 调用__getitem__(self, key)myclass[key] = value    # 调用__setitem(self, key, value)\n\n__getslice__(self, begin, end) 和 __setslice__(self, begin, end, value)使得对象可以使用[begin: end]这样类似list的方式进行获取和设置。\n__iter__(self)通过在函数内返回迭代器，使得对象可以被循环。\n__add__(self, other)实现加法操作。对于其他运算也有相应的魔法方法\n__lt__(self, other)x&lt;y相当于调用x.lt(y)，定义了使用&lt;时的评判标准。对于其他比较运算，也有对应的魔法方法。\n","categories":["Programming Language","Python"],"tags":["概念","python"]},{"title":"Python全局解释器锁GIL","url":"/2021/05/20/Python%E5%85%A8%E5%B1%80%E8%A7%A3%E9%87%8A%E5%99%A8%E9%94%81GIL/","content":"GIL全局解释器锁GIL和Python其实并没有多大关系。因为Python是解释型语言，所以需要解释器，那么在最常见的C语言编写的CPython中，有一个部件会限制多线程的性能，这个部件就是GIL。\n在CPython运行的Python中，多线程实际上是不存在的，比如CPU有两个核，py文件中开了两个线程，那么实际上CPU的2个核利用率都为50%，即两个核实际上是交替运行线程的，即串行执行任务而非并行。这么做的原因就是因为GIL限制了每一次一个进程只能执行一个线程。\n因此，在Python中想要实现真正的并行，需要使用多进程而非多线程。\n","categories":["Programming Language","Python"],"tags":["概念","python"]},{"title":"Python方法解析顺序表MRO","url":"/2021/05/21/Python%E6%96%B9%E6%B3%95%E8%A7%A3%E6%9E%90%E9%A1%BA%E5%BA%8F%E8%A1%A8MRO/","content":"MRO主要是解决多继承中重复继承的问题。\n在使用父类名.__init__()的方法中，会造成多次继承父类的父类，如下图所示。\n\n为了避免这样的问题，可以使用super，该方法基于MRO，根据MRO的顺序决定继承的顺序。\n\n使用className.__mro__可以查看MRO链，使用super().__init__()方法时会在MRO链中选中当前class，然后调用下一个class的__init__方法。\n也可以使用super(class, self).__init__()指定调用的下一个class的__init__方法。\n","categories":["Programming Language","Python"],"tags":["概念","python"]},{"title":"ab并发测试","url":"/2021/05/12/ab%E5%B9%B6%E5%8F%91%E6%B5%8B%E8%AF%95/","content":"什么是abab是Apache HTTP server benchmarking tool的缩写，可以用以测试HTTP请求的服务器性能,也是业界比较流行和简单易用的一种压力测试工具包。\n使用ab在终端中切换至安装ab的bin目录下，然后输入ab -n &lt;requestNum&gt; -c &lt;concurrencyNum&gt; &lt;destination&gt;\n\nrequestNum: 表示发送请求的数量\nconcurrencyNum: 表示并发请求的数量\ndestination: 表示请求的目标\n\n\n注：如果提示 SSL not compiled in; no https support，请使用abs代替ab\n\nab中常见的概念吞吐率(Requests per second)用于描述服务器处理并发的能力，指的是某个并发用户数量下单位时间内处理的请求数量。\n并发连接数(The number of concurrent connections)某个时刻服务器所接受的请求数目\n并发用户数(Concurrency level)由于一个用户可能发起多个请求，所以并发用户数≤并发连接数\n用户平均等待时间处理完所有请求的时间 / (总请求数 / 并发用户数)\n服务器平均请求等待时间处理完所有请求的时间 / 总请求数 = 吞吐率-1 = 用户平均请求等待时间 / 并发用户数\n","categories":["Server"],"tags":["high concurrency","server"]},{"title":"Redis基础","url":"/2021/05/17/Redis%E5%9F%BA%E7%A1%80/","content":"NoSQL 数据库NoSQL (No only SQL)是一个高性能的 key-value 非关系型数据库。\nRedis 优势Redis 有众多优势：\n\n性能好：可以用于减小CPU压力和数据库的读写压力\n数据类型丰富\n原子性：即单个操作是原子性的，要么成功要么完全不执行；多个操作可以使用事务完成原子性操作\n注：Redis 支持事务，但是和 MySQL 的事务不同，并不支持ACID特性。\n\n\n持久化数据库：运行在内存，但是可以将数据持久化到磁盘\n\nRedis 主要适用于“高并发、大数据、可扩展”的情形，但是不适用于“需要大量事务操作和结构化查询”的任务中。\nredis VS memcacheRedis 和常用的 memcache 有很大的不同：\n\nmemcache 不能持久化，只能将数据保存在内存中，断电后数据消失\nmemcache 没有数据类型概念，存的是json\nmemcache 使用 “串行+多线程+锁” 的方式实现并发，而 redis 使用 “单线程+多路IO复用” 实现并发\n\n\n为什么细分类型比使用Json好？\n\n如果使用json想要取回某一条数据时，需要返回整个Json，代价高\n计算向数据转移：使用json时获取数据需要在client进行解读；而细分数据类型时计算在server端完成，client直接得到计算完的结果\n使用json时，类似使用某种未知语言通信，用户看不懂就要放入各种语言的翻译器尝试理解；细分数据类型后，相当于告知会使用什么语言通信，用户直接使用该语言的翻译器就可以理解语义\n\n\n\n\n为什么 redis 支持高并发并效率高\nredis 基于内存，免去了磁盘读写，减小了DB的IO压力\nredis 单线程，不受CPU的限制，也省去了上下文切换\nredis 多路复用，可以处理并发请求 (使用了epoll)\n\n\nRedis 数据类型和基本操作数据库设置Redis 默认有16个库\n\nselect &lt;dbid&gt;来进行数据库切换\nkeys *查看所有的keys\nexists &lt;key&gt;查看某个key是否存在\ndel &lt;key&gt;删除某个key\nunlink &lt;key&gt;异步删除某个key\ntype &lt;key&gt;查看key对应数据的数据类型\nobject encoding &lt;key&gt;查看key对应数据的编码方式/实现方式\nexpire &lt;key&gt; &lt;time&gt;设置某个key的生命周期\nttl &lt;key&gt;查看当前剩余时间，-1为永不过期，-2为已过期\n\n\n\ntype – 返回value的数据类型，只有String、List、Sorted List、Hash和Set这五种\nobject – 返回实现方式，比如embstr、int、raw等类型。redis默认会先以节省内存的编码方式创建，当满足一定条件时，会转换为更为通用的编码方式。\n\n\nString是二进制安全的一种数据类型，可以存储任何“可转换为string类型”的数据，其底层是SDS(simple dynamic string，简单动态字符串)。该数据结构会预先分配一定的capacity，然后在空间不足时分配更多capacity。\n\n注：capacity最大为512MB，小于1MB时空间进行翻倍，大于1MB时每次多分配1MB\n\n二进制安全二进制安全指的是redis基于字节流而非字符流，所有的内容都会转换成没有实际意义的二进制来存储。比如在C语言中有 “hello\\0world”，那么基于字符流时，会识别\\0，因此只能保存 “hello”；而基于字节流时，所有内容转换为二进制，\\0变为一串01序列，所以可以存储 “hello\\0world”。\n基本操作\nset &lt;key&gt; &lt;value&gt;设置键值对\nsetnx &lt;key&gt; &lt;value&gt;在key不存在时才设置\nget &lt;key&gt;获取值\nappend &lt;key&gt; &lt;str&gt;将str附加到值的尾部\nstrlen &lt;key&gt;获取值的长度&gt; set k1 100&gt; strlen k13&gt; append k1 A&gt; strlen k14   # “100A”&gt; set k2 中&gt; get k2# 此处的返回要根据预设的编码方式决定# 如果是GKB，则是2个hex# 如果是UTF8，则是3个hex# 如果设置了`redis-cli --raw`，则返回&quot;中&quot;&gt; strlen k2# 同上，需要根据编码方式决定\nincr/decr &lt;key&gt;值++/–\nincrby/decrby &lt;key&gt; &lt;step&gt;值+step/-step\nmset &lt;k1&gt; &lt;v1&gt; &lt;k2&gt; &lt;v2&gt; ...设置多个键值对(原子操作，任何一个失败则全部失败，不予设置)\nmget &lt;k1&gt; &lt;k2&gt;获取多个值\ngetrange &lt;key&gt; &lt;start&gt; &lt;end&gt;取key[start, end]，注意两边界可取\nsetrange &lt;key&gt; &lt;start&gt; &lt;value&gt;在start处插入一个value\ngetset &lt;key&gt; &lt;value&gt;获取旧值，设置新值\n\nList单键多值，底层为双向链表(quickList)，在数据量少时，使用一块连续内存，称为zipList，在数据量多时，将多个zipList作为节点，构建双向链表，称为quickList。\n常见操作\nlpush/rpush &lt;key&gt; &lt;v1, v2, ...&gt;从左侧/右侧压入数据\nrpoplpush &lt;k1&gt; &lt;k2&gt;从k1的右侧弹出元素放入k2的左侧\nlrange &lt;key&gt; &lt;start&gt; &lt;end&gt;key[start, end]\nlindex &lt;key&gt; &lt;index&gt;key[index]\nllen获取长度\nlinsert &lt;key&gt; before/after &lt;value&gt; &lt;newvalue&gt;在key[value]前/后插入一个newvalue\nlrem &lt;key&gt; &lt;n&gt; &lt;value&gt;\nblpop &lt;key&gt; &lt;timeout&gt; – 在timeout时间内阻塞等待弹出\n\n可实现的数据类型\n栈 – 通过同向插入删除即可\n队列 – 通过反向插入删除即可\n数组 – 使用LSET和LINDEX对单数据操作\n阻塞单播队列 – 使用BLPOP和BRPOP这种阻塞操作实现\n\nSet与list相似，但是不允许重复，底层是字典(hash表)，增删查改都是O(1)。\n\nsadd &lt;key&gt; &lt;v1, v2, ...&gt;\nsmembers &lt;key&gt;\nsismember &lt;key&gt; &lt;value&gt;if value in key?\nscard &lt;key&gt;返回个数\nsrem &lt;key&gt; &lt;v1, v2, ...&gt;删除\nspop &lt;key&gt;随机弹出一个\nsrandmember &lt;key&gt; &lt;n&gt;随机抽取n个\nsmove &lt;k1&gt; &lt;k2&gt;从k1移动一个到k2\nsdiffstore/sinterstore/sunionstore &lt;destkey&gt; &lt;k1&gt; &lt;k2&gt;将k1与k2进行操作放入destkey\nsdiff/sinter/sunion &lt;key ...&gt;操作后输出\n\nHash存储的是 {key: {field: value, field: value}} 的结构。\n基本操作\nhset &lt;key&gt; &lt;field&gt; &lt;value&gt;\nhget &lt;key&gt; &lt;field&gt;\nhmset &lt;key&gt; &lt;field1&gt; &lt;value1&gt; &lt;field2&gt; &lt;value2&gt;...\nhexists &lt;key&gt; &lt;field&gt;\nhvals/hkeys &lt;key&gt;查看所有value/key\n\nZset与set类似，但是是有序的，因为其为每个成员设置了一个score用于排序。底层使用了两种数据结构：\n\nhash: {key: {value: score}}\n跳表\n\n\nzadd &lt;key&gt; &lt;s0&gt; &lt;value0&gt; &lt;s1&gt; &lt;value1&gt; ...\nzrange &lt;key&gt; &lt;start&gt; &lt;end&gt; [withscores]\nzrangebyscore &lt;key&gt; &lt;min&gt; &lt;max&gt; [withscores]从大到小排序\nzrevrangebyscore ...反序\nzcount &lt;key&gt; &lt;min&gt; &lt;max&gt;范围内的元素个数\nzrank &lt;key&gt; &lt;value&gt;返回排名，从0开始计数\nzrem &lt;key&gt; &lt;value&gt;删除\n\nBitmaps实际上是字符串，但可以视为只能存储0和1，其下标被称为“偏移量”，用于提高内存使用率。\n基本操作\nsetbit &lt;key&gt; &lt;offset&gt; &lt;value&gt;(注意是从最左边开始算offset，且STRLEN返回的是字节数而非比特数)&gt; setbit k1 1 101000000            # 从左边开始算偏移&gt; STRLEN k11                   # 返回字节数&gt; setbit k1 1 90100000001000000    # 仍然是从最左边开始算偏移 &gt; STRLEN k12                   \ngetbit &lt;key&gt; &lt;offset&gt;\nbitop and/or/not/xor &lt;destkey&gt; &lt;key&gt; [key...]\nbitcount &lt;key&gt; [start] [end]用于记录为1的个数，注意此处是针对8个bits，即一个byte为单位\nbitpos &lt;key&gt; &lt;bit&gt; [start] [end]用于查询第start~第end个字节内第一个bit元素的偏移量(仍然是从最左边开始计算偏移量)&gt; set k1 1 1&gt; set k1 9 1&gt; get k101000000 01000000&gt; bitpos k1 1 0 01   # 第0个字节内第一个1的偏移量&gt; bitpos k1 1 1 19   # 第1个字节内第一个1的偏移量，注意仍然是从最左侧计算偏移，所以是9而非1\n\n使用场景\n统计登录天数\n\n每一个key代表一个用户，value中保存了365个bit，用于代表每一天。该用户在第i天登录，那么setbit user i 1，即把第i个bit设置为1代表登录了。\n如果想要知道某个用户user的登录天数，那么只需要bitcount user 0 -1即可。\n\n统计活跃用户\n\n每一个key代表一天，value中每一个bit代表一个用户。如果用户i在当天登录，那么setbit date i 1。\n如果定义每一天都登录的为活跃用户，那么只需要bitop and [date ...]，再统计最后结果中有多少个为1即代表由多少活跃用户。\nHyperLogLog用于做基数统计。在输入数据巨大时，计算基数所需的空间总是固定且较小的，比如在Redis中可以用12KB计算264个基数。\n\n基数：不重复的数字个数\n\n\npfadd &lt;key&gt; &lt;element ...&gt;\npfcount &lt;key&gt;\npfmerge &lt;destkey&gt; &lt;key1, key2 ...&gt;将多个Log进行合并\n\nGEO主要用于保存地理信息，可以计算直线距离等\n\nredis-benchmark 压测使用redis-benchmark可以进行压力测试。\n\n-h – 测试的主机ip\n-p – 端口\n-c – 并发数(concurrency)\n-n – 请求数\n\n\nLua脚本Lua是一种非常轻量级的脚本语言，一般使用C语言编写，其设计目的是为了嵌入应用程序中，从而为应用程序提供灵活的扩展和定制功能。\n为什么要使用Lua\n减少网络开销(类似pipeline)：多个请求通过脚本一次发送，减少网络延迟\n原子操作(类似事务)：将脚本作为一个整体执行，中间不会插入其他命令，无需使用事务\n复用：客户端发送的脚本永久存在redis中，其他客户端可以复用脚本\n可嵌入性：可嵌入JAVA，C#等多种编程语言，支持不同操作系统跨平台交互\n\n如何使用Lua\nEVAL script numkeys key [key ...] arg [arg ...]\n\n其中script是脚本的内容或路径，numkeys为传入的key的个数，然后key和arg分别对应Lua脚本中的 KEYS[1] KEYS[2] … 和 ARGV[1] ARGV[2] …\n\nSCRIPT LOAD script\n\n返回脚本的SHA1校验和，不执行\n\nEVALSHA sha1 numkeys key [key ...] arg [arg ...]\n\n根据传入的 sha1 校验和执行脚本\n\n权限控制ACL(Access Control List)访问控制列表：允许设置可执行的命令和可访问的键来限制用户操作，从而对用户进行更细粒度的权限控制\n\nacl list – 列出用户信息和权限\nacl setuser – 新建用户，并可以设置其权限\nacl cat – 查看指令\nacl whoami – 显示当前用户\nauth &lt;username&gt; &lt;password&gt; – 切换用户\n\n\n","categories":["Redis"],"tags":["redis"]},{"title":"caffe2pytorch","url":"/2021/04/20/caffe2pytorch/","content":"Caffe2PyTorchThis cookbook is a guide about how to transfer Caffe layers to PyTorch functions and networks.\nGuidance\nConv\nBN\nReLU\nReshape\nTranspose\nMatrix Mul\nSoftmax\nCrop\nEltwise\nDeconV\nConcat\n\n\nnn.Conv2dnn.Conv2d(in_channels, out_channels, kernel_size, padding, stride, bias)\nlayer &#123;  name:   type: &quot;Convolution&quot;  bottom:   top:   convolution_param &#123;    num_output: 64    bias_term: true    pad: 3    kernel_size: 7    stride: 2    weight_filler &#123;      type: &quot;msra&quot;    &#125;  &#125;&#125;\n\n\nnn.BatchNorm2dnn.BatchNorm2d(out_channels)\nlayer &#123;  name:   type: &quot;BatchNorm&quot;  bottom:   top:   batch_norm_param &#123;    use_global_stats: false  &#125;&#125;layer &#123;  name:   type: &quot;Scale&quot;  bottom:   top:   scale_param &#123;    bias_term: true  &#125;&#125;\n\n\nnn.ReLUnn.ReLU(inplace=True)\nlayer &#123;  name:   type: &quot;ReLU&quot;  bottom:   top: &#125;\n\n\ntorch.reshapetorch.reshape(x, (n, c, -1)) # caffe中dim=0表示维度不变\nlayer &#123;  name:   type: &quot;Reshape&quot;  bottom:   top:   reshape_param &#123;    shape &#123;      dim: 0      dim: 0      dim: -1    &#125;  &#125;&#125;\n\n\ntorch.transposetorch.transpose(x, dim_0, dim_1)    # torch的transpose只能一次交换两个维度，多个维度需要多次交换\nlayer &#123;  name:   type: &quot;TensorTranspose&quot;  bottom:   top:   tensor_transpose_param &#123;    order: 0    order: 2    order: 1  &#125;&#125;\n\n\ntorch.bmmtorch.bmm(x, y) # bmm针对batch做矩阵乘法，即(n, c, w, h)，如果是(c, w, h)则可以使用bm\nlayer &#123;  name:   type: &quot;MatrixMultiplication&quot;  bottom:   bottom:   top: &#125;\n\n\nF.softmaxF.softmax(x, dim)   # dim即为下面的axis\nlayer &#123;  name:   type: &quot;Softmax&quot;  bottom:   top:   softmax_param &#123;    axis: 2  &#125;&#125;\n\n\nCropclass Crop(nn.Module):    &#x27;&#x27;&#x27;        @ axis      -&gt;      从axis开始裁减后面的维度        @ offset    -&gt;      裁减的时候要偏离多少距离        ! 注意：在Caffe中，是以B的大小来裁减A，即 B.shape &lt;= A.shape    &#x27;&#x27;&#x27;    def __init__(self, axis, offset=0):        super(Crop, self).__init__()        self.axis = axis        self.offset = offset    def forward(self, x, ref):        for axis in range(self.axis, x.dim()):            ref_size = ref.size(axis)            indices = torch.arange(self.offset, self.offset + ref_size).long()            indices = x.data.new().resize_(indices.size()).copy_(indices)            x = x.index_select(axis, indices.to(torch.int64))        return x\nlayer &#123;  name:   type: &quot;Crop&quot;  bottom: A  bottom: B  top:   crop_param &#123;    axis: 2  &#125;&#125;\n\n\nEltwiseclass Eltwise(nn.Module):    def __init__(self, operation=&#x27;+&#x27;):        super(Eltwise, self).__init__()        self.operation = operation    def forward(self, *inputs):        if self.operation == &#x27;+&#x27; or self.operation == &#x27;SUM&#x27;:            x = inputs[0]            for i in range(1,len(inputs)):                x = x + inputs[i]        elif self.operation == &#x27;*&#x27; or self.operation == &#x27;MUL&#x27;:            x = inputs[0]            for i in range(1,len(inputs)):                x = x * inputs[i]        elif self.operation == &#x27;/&#x27; or self.operation == &#x27;DIV&#x27;:            x = inputs[0]            for i in range(1,len(inputs)):                x = x / inputs[i]        elif self.operation == &#x27;MAX&#x27;:            x = inputs[0]            for i in range(1,len(inputs)):                x =torch.max(x, inputs[i])        else:            print(&#x27;forward Eltwise, unknown operator&#x27;)        return x\n\nlayer &#123;  name:   type: &quot;Eltwise&quot;  bottom:   bottom:   top: &#125;layer &#123;  name:   type: &quot;Eltwise&quot;  bottom:   bottom:   top:   eltwise_param &#123;    operation: PROD  &#125;&#125;\n\n\nnn.ConvTranspose2dnn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, output_padding, groups, bias)\nlayer &#123;  name:   type: &quot;Deconvolution&quot;  bottom:   top:   param &#123;    lr_mult: 0.0  &#125;  convolution_param &#123;    num_output: 128    bias_term: false    pad: 0    kernel_size: 4    group: 128    stride: 2    weight_filler &#123;      type: &quot;bilinear&quot;    &#125;  &#125;&#125;\n\n\nConcatclass Concat(nn.Module):    def __init__(self, axis):        super(Concat, self).__init__()        self.axis = axis    def forward(self, *inputs):        return torch.cat(inputs, self.axis)\n\nlayer &#123;  name:   type: &quot;Concat&quot;  bottom:   bottom:   bottom:   top:   propagate_down: true  propagate_down: true  propagate_down: false  concat_param &#123;    concat_dim: 1  &#125;&#125;\n","categories":["Deep Learning","Framework","PyTorch"],"tags":["Deep Learning","PyTorch"]},{"title":"WSGI","url":"/2021/04/26/WSGI/","content":"什么是WSGIWSGI规范的目的就是解耦 Web Server 和 Web Application。 一个完整的WSGI协议包括 server 和 appliction 两部分。\n其中，像 gunicorn, uwsgi 就是实现的 server 端，而 Django, Flask 则是实现的 application 端。这样一来，我们就可以像搭积木一样随意组合web server 和 web framework了。\n\nWSGI规定，Web程序必须有一个可调用对象，且该可调用对象接收两个参数，返回一个可迭代对象：\n\nenviron：字典，包含请求的所有信息\nstart_response：在可调用对象中调用的函数，用来发起响应\n\n下面实现一个最简单的 application :\ndef helloWorld(environ, start_response):    status_code = &#x27;200 OK&#x27;    header = [(&#x27;Content-Type&#x27;, &#x27;text/html&#x27;)]    start_response(status_code, header) # 用于发送HTTP header    body = &#x27;&lt;h1&gt;Hello, %s!&lt;/h1&gt;&#x27; % (environ[&#x27;PATH_INFO&#x27;][1:] or &#x27;World&#x27;)    return [body.encode(&#x27;utf-8&#x27;)]\n\n然后再编写一个 server 函数：\n# 从wsgiref模块导入:from wsgiref.simple_server import make_server# 导入application函数:import helloWorld# 创建一个服务器，IP地址为空，端口是8000，处理函数是helloWorld:httpd = make_server(&#x27;127.0.0.1&#x27;, 8000, helloWorld)# 开始监听HTTP请求:httpd.serve_forever()\n\n\n如果输入的url为127.0.0.1/那么输出的是Hello World；如果输入的url为127.0.0.1/kexin那么输出的是Hello kexin。\n\n\n在 Django 中，有一个wsgi.py定义了上述的相关内容：\n&#x27;&#x27;&#x27;    wsgi.py&#x27;&#x27;&#x27;import osfrom django.core.wsgi import get_wsgi_applicationos.environ.setdefault(&#x27;DJANGO_SETTINGS_MODULE&#x27;, &#x27;config.settings&#x27;)  # 从setting读取参数初始化WSGI# 将用户编写的代码整合成一个application# 这样web server调用此处的application就相当于上面调用helloWorld一样application = get_wsgi_application()    \n\n","categories":["Web","Python"],"tags":["python"]},{"title":"hexo-pdf","url":"/2021/04/18/hexo-pdf/","content":"If you want to insert .pdf files in your HEXO pages, there are two methods.\n\nMETHOD 0\n\nYou can install HEXO package hexo-pdf.\nnpm install --save hexo-pdf\n\nThen, you can use .ejs syntax &#123;% pdf path %&#125; to achieve function.\n&#123;% pdf https:&#x2F;&#x2F;drive.google.com&#x2F;xxx %&#125;  # external url&#123;% pdf .&#x2F;xxx.pdf %&#125;                     # file path\n\n\nMETHOD 1\n\nIn your index.md, you can use html syntax to achieve this function.\n&lt;object data=&quot;https://drive.google.com/xxx&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;&lt;object data=&quot;./xxx.pdf&quot; type=&quot;application/pdf&quot; width=&quot;100%&quot; height=&quot;100%&quot;&gt;\n\n\nYou can adjust the width and height even other properties.\n\n","categories":["Web","HEXO","Framework","HEXO"],"tags":["web","hexo","package"]},{"title":"编译型语言和解释型语言","url":"/2021/04/19/%E7%BC%96%E8%AF%91%E5%9E%8B%E8%AF%AD%E8%A8%80%E5%92%8C%E8%A7%A3%E9%87%8A%E5%9E%8B%E8%AF%AD%E8%A8%80/","content":"编译型语言和解释型语言计算机是不能理解高级语言的，更不能直接执行高级语言，它只能直接理解机器语言，所以使用任何高级语言编写的程序若想被计算机运行，都必须将其转换成计算机语言，也就是机器码。而转换主要有两种方法\n\n编译 – 先进行转换，然后再运行\n\n解释 – 运行的过程中进行转换\n\n\n编译型语言使用专门的编译器，针对特定的平台，将高级语言源代码一次性的编译成可被该平台硬件执行的机器码，并包装成该平台所能识别的可执行性程序的格式(如.exe)。由于已经生成了可执行格式，以后运行时不需要编译，所以编译型语言执行效率高(编译一次，多次使用)。\n一般而言，编译型语言有以下特点：\n\n编译后无需再次编译，性能好\n针对特定平台，跨平台性能差\n\n由于有以上的特点，所以编译型语言常用于OS，嵌入式等对性能要求极高的特定平台，所以衍生出的经典语言类型有 – C/C++\n解释型语言使用专门的解释器对源程序逐行解释成特定平台的机器码并立即执行。是代码在执行时才被解释器一行行动态翻译和执行，而不是在执行之前就完成翻译。\n一般而言，解释型语言有以下特点：\n\n由于在执行时再翻译，每次执行都要翻译，效率低\n只要对应平台有相应的解释器即可运行，跨平台性能好，便于移植\n\n由于有以上的特点，所以解释型语言常用于Web，脚本等跨平台使用的场景，所以衍生出的经典语言类型有 – Python, Javascript\n关于JavaJava是一种很特殊的语言，其需要将.java文件通过编译生成.class文件，但是该.class文件却可以在各种平台上使用，因为java针对不同的平台有不同的JVM，实现了跨平台。\n\n\n你可以说它是编译型的：因为所有的Java代码都是要编译的，.java不经过编译就什么用都没有。 \n你可以说它是解释型的：因为java代码编译后不能直接运行，它是解释运行在JVM上的，所以它是解释运行的，那也就算是解释的了。 \n但是，现在的JVM为了效率，都有一些JIT优化。它又会把.class的二进制代码编译为本地的代码直接运行，所以，又是编译的。\n\n\n\n为什么需要编译编译有两个主要的好处：\n\n可以提前排除错误;\n对于Java而言，编译并不会直接像C++一样产生基于特定系统的机器码，而是会产生一个二进制比特文件(又称为class文件)，这个文件可以在多种平台被识别(通过JVM)，然后再基于特定的系统进一步进行翻译，实现跨平台与高效率的trade-off\n\n","categories":["Programming Language","Java"],"tags":["概念","Java"]},{"title":"Redis进阶","url":"/2021/05/19/Redis%E8%BF%9B%E9%98%B6/","content":"事务&amp;锁Redis 事务是一个单独的隔离操作，所有命令序列化、按序执行，不会被插手打断。之所以要引入事务，就是为了将多个小操作合并成一个大操作，且“一次性”执行完整个大操作。\n基本步骤\nmulti – 用于组合小操作，此时只是组合，并未执行。如果出现错误，则所有的操作均被抛弃，无法运行任何一个。\nexec – 执行队列。如果有错误，则只会执行正确的步骤，并不会像MySQL一样回滚。\ndiscard – 放弃之前的操作。\n\n解决冲突由于现实生活中都是并发执行的，所以可能出现多个对同一资源的诉求，这被称作冲突。解决冲突的方法就是加锁。\n乐观锁乐观锁认为资源不会轻易受污染，所以放任多个线程同时访问，如果访问结束后出现冲突再解决。\n具体方法是：使用版本号，对资源操作后更新版本号，其他已经占用资源的线程如果想更改资源，发现版本号不同后就无法操作。\n悲观锁悲观锁认为资源很容易就被污染，所以每一步操作都要加以限制。\n具体方法就是：一旦某个进程控制了资源，就给他加上锁，别的进程就只能等待。当使用完资源后，进程再解锁，别的进程再开始竞争。\n其逻辑简单，但是相当于是串行实现多线程，效率低下。\n代码实现使用multi开启组队模式，组队完成后可以使用exec进行执行或discard放弃组队。而 Redis 中是不支持直接使用悲观锁的，所以加锁都是乐观锁。具体操作就是使用watch &lt;key&gt;完成对某个数据的监视，在事务执行前，如果被监视的数据发生了变化，则事务不能执行。\n三大特性\n单独隔离操作\n无隔离级别\n不保证原子性 – 只能保证不被打断，一旦出错是不会rollback的\n\n\nPipelineRedis客户端执行一条命令分为以下四个步骤:\n\n发送命令\n命令排队\n命令执行\n返回结果\n\n其中,第一步+第四步称为Round Trip Time(RTT,往返时间)。\n如果想要执行多个步骤，则需要耗费多个RTT，如果能把这些步骤在一个RTT内一起发送，就会非常有效率。这样的实现机制就是Pipeline。\nPipeline和事务的差别\nPipeline是将命令打包好一起发送，服务器接受后再一条一条的执行，处理完再将这些命令的结果一并返回；但是事务是在组队后发送多个操作，服务器在组队完毕后一口气执行完，最后再返回结果。(pipeline是将命令打包一次发送，服务器多次执行；事务是命令分开发送，服务器一并执行)\npipeline只不过是将命令一起发送，对服务器而言是不知道客户端是否使用了pipeline；而事务是服务器支持的，客户端的命令发送仍然是分离的，但是服务器会将组队的一起执行。(pipeline是客户端行为，事务是服务端行为)\n执行事务时会阻塞其他任务，而pipeline不会\n事务是原子性的，pipeline不是\n\n\n持久化RDBRDB (Redis Database)是在指定时间间隔内将数据快照(snapshot)从内存中转移到磁盘中，恢复时将快照转至内存中。\nRedis中RDB的配置默认在开启服务器的路径下生成一个dump.rdb文件，用于保存快照，其中可以设置时间间隔和达到的目标，如3600秒内更新了100个数据才会进行持久化。\n\n如果3600s内更新了101个文件，那么前100个会进行持久化，多余的1个会算入下一个3600s内。\n\n还有一些设置是否自动持久化、磁盘无法写入是否终止等操作的，可以自己摸索。\n如何持久化Redis会单独创建一个子进程用于持久化，会先将数据写入一个临时文件，当持久化结束后再用临时文件替换上一次持久化的文件。整个过程中，主进程不进行任何IO操作。\n由于需要将临时文件全部写入后才会更换原来的持久化文件，如果当写临时文件时出现宕机等，导致临时文件没有写完，那么持久化文件仍保存了上一次的数据，新的数据就丢失了。\n优缺点优点：\n\n适合大规模数据，因为只需要父进程fork一个子进程，然后就不用管子进程\n节省磁盘空间\n恢复速度快\n\n缺点：\n\nfork需要生成临时文件，会占用额外空间\n最后一次持久化后的数据可能丢失\n虽然使用写时拷贝技术，但是如果数据庞大时还是比较消耗性能\n\nAOFAOF (Append Only File)会以日志形式记录每个写操作(增量保存，不会记录读操作)，只许追加文件但不可以改写文件。Redis启动时会将日志内容重新按序执行一次以恢复/加载数据。\n默认情况下AOF是不开启的，如果RDB和AOF同时开启，Redis会选择使用不会丢失信息的AOF。\nAOF同步频率\nappendfsync always：始终同步，每次写入操作都会记录到日志中，性能差但是完整性好\nappendfsync everysec：每秒同步一次\nappendfsync no：不主动同步，同步的时机由OS控制\n\nRewrite数据压缩将多个步骤压缩成较少的步骤，如set k1 v1和set k2 v2合并成mset k1 v1 k2 v2进行保存。\n当AOF文件持续增长而过大时，会fork一个新的进程来重写文件(先创建临时文件再覆盖)，实际上就是把rdb的快照以二进制形式加在新的aof的头部，作为历史数据汇总，替换流水账。\n流程\n客户端写命令被追加到AOF缓冲区\nAOF缓冲区根据同步的频率将内存&rarr;磁盘\nAOF文件超过大小时进行rewrite\nRedis下次启动时会加载AOF的写操作完成数据加载\n\n优缺点优点：\n\n备份机制更加稳健，丢失数据概率低\n日志文件易读，且有日志文件纠错\n\n缺点：\n\n占用磁盘空间\n恢复和加载数据慢\n同步频率设置过高会造成巨大压力\n\n选择RDB还是AOF？\n建议两者都开启，搭配使用\n如果不追求数据完整性，建议使用RDB\n不建议单独使用AOF，因为可能存在日志BUG无法恢复数据\n如果只是做内存缓存，则可以都不使用\n\n\n主从复制主机数据更新后根据配置和策略，自动同步到备机的master/slave机制，Master以写为主，Slave以读为主\n为什么要使用主从复制\n读写分离，提高效率\n容灾快速恢复：由于一般有多个从服务器(即可以从多个地方读)，所以一旦一个服务器宕机，还可以依赖别的服务器\n\n步骤\n新建多个 conf 文件\n设置 pid (windows不适用)、rdb filename、port 和 slaveof [masterIP] [masterPORT]\n启动服务redis-server &lt;conf&gt;\n启动客户端redis-cli -p &lt;port&gt;，在客户端中使用info replication可以查看主从情况\n\n如何实现数据同步\nSlave 连接上 Master 后，主动向 Master 发送请求，要求进行数据同步(Slave 主动)\nMaster 收到 Slave 请求后，进行持久化，生成rdb文件，然后把rdb文件发送给 Slave\n全量复制：Slave 通过rdb进行数据同步\n增量复制：当有请求向 Master 进行write操作，Master 处理完后发送给 Slave 进行数据同步（Master 主动）\n\n四种常用的主从方式一主多从如果从机挂了重启，数据恢复会将主机中的所有数据进行复制(sync)；如果主机挂了，从机会持续尝试和主机同步，直到主机上线。\n薪火相传构成树形结构，中间的服务器对于父节点而言是从机，对于子节点而言是主机。\n\n存在很大的问题，中间服务器宕机，则其所有从机都要停止工作\n\n反客为主如果主机挂了，可以设置一个从机为新的主机；旧的主机重启后变为从机。\n\n需要手动使用slaveof no one进行设置。这也是这种方法的缺点，需要手动控制\n\n哨兵模式相当于自动版的“反客为主”：主机挂了会自动推选一个从机。\n由于有多个从机，推举哪一个就需要有设置：\n\n设置优先级replica-priority &lt;value&gt;，value越小，优先级越高\n如果优先级一样，则选择偏移量(获取主机数据更安全的/同步率最高的)大的\n如果还是一样，则选择runid小的(随机)\n\n步骤\n新建 sentinel.conf\n设置sentinel monitor &lt;name&gt; &lt;masterIP&gt; &lt;masterPORT&gt; &lt;num&gt;\n\nname – 别名\nnum – 至少num个从机同意推举新的主机后才能推举\n\n\n\n启动哨兵模式redis-server &lt;conf&gt; --sentinel\n\n\n集群使用集群可以解决很多问题：\n\n容量不够，如何扩容redis\n提高并发量\n如果服务器挂了重启，ip可能会变。之前使用“代理主机”的方法解决，如今使用“去中心化集群”的方法解决\n\n代理主机 &amp; 去中心化集群代理主机使用代理服务器对请求进行分流\n                                    ---&gt;  server0client  --- request ---&gt;  代理(中心)  ---&gt;  server1                                    ---&gt;  server2\n\n去中心化集群每一台服务器都可以用作“代理服务器”\nclient  --- request ---&gt;  server0  &lt;---&gt;  server1                            |                |                            |                |                          server2 ------------\n\n\n水平扩容：N个redis节点可以将整个DB分解成N份，每个节点只需要存储一小部分\n可用性扩展：即使有一部分挂了，其他部分也可以正常运行\n\n使用集群注意，在Redis集群模式下，至少需要三个主机，且在分配服务器为主机的时候，会采用“主机IP尽量不同，从机和主机IP也尽量不同”的原则。\n配置文件在 conf 文件中设置如下内容：\n\ncluster-enabled yes\ncluster-config-file [file]\ncluster-node-timeout [time]\n\n开启服务器组队redis-cli --cluster create --cluster-replicas 1 [ip: port]\n\n上述代码中的1代表了配置方式，其中1表示一主一从\n\n开启客户端redis-cli -c -p [port]\n\n-c代表以集群方式开启\n\n在客户端中可以使用cluster nodes查看集群信息\nslot一个Redis集群有16384个slots，数据库中的key-value都属于某一个slot中。集群使用CRC(key)%16384来hash分配slot，集群中每个主机负责一个区域的slot (如主机1负责slot号从0到999的) 。\n注意事项\n在集群模式下插入多值时，需要使用{}# 不能使用 mset k1 v1 k2 v2 ...mset k1&#123;key&#125; v1 k2&#123;key&#125; v2 ...\ncluster中有特定的函数cluster keyslot &lt;key&gt;           # 查询key所属的slot# 下面两个操作必须要在管理指定slot的主机上查询才行cluster countkeysinslot &lt;slot&gt;  # 查询slot内的key数量cluster getkeysinslot &lt;slot&gt;    # 获取slot内key的名称\n通过设置cluster-require-full-coverage为yes可以在集群中某个节点主从机全部宕机时命令整个集群失去作用；否则集群的其他节点仍可使用。\n\n\nRedis常见的问题缓存穿透key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会到数据源，从而可能压垮数据源。\n具体的流程：应用服务器压力突增 &rarr; request请求的都是redis中没有的，redis命中率下降，只能查询DB &rarr; DB压力突增，被穿透\n解决方法\n\n对空值进行缓存，并设置比较短的过期时间\n使用Bitmap设置白名单\n布隆过滤器\n实时监控，在出现缓存穿透时通知工作人员，禁用某些一直攻击的黑客\n\n\n布隆过滤器\n\n理念：类似一个hash set，用来判断某个key是否在某个集合中。和一般的hash set不同的是，这个算法无需存储key的值，对于每个key，只需要k个比特位，每个存储一个标志，用来判断key是否在集合中。\n操作\n首先需要k个hash函数，每个函数可以把key散列成为1个整数\n初始化时，需要一个长度为n比特的数组，每个比特位初始化为0\n某个key加入集合时，用k个hash函数计算出k个散列值，并把数组中对应的比特位置为1\n判断某个key是否在集合时，用k个hash函数计算出k个散列值，并查询数组中对应的比特位，如果所有的比特位都是1，认为在集合中\n\n\n优点：不需要存储Key，只需要二进制bit表示\n缺点：(1)无法删除；(2)算法判断key在集合中时，有一定的概率key其实不在集合中\n\n\n缓存击穿key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把DB压垮\n具体的流程：应用服务器压力突增 &rarr; redis正常运行，多个request请求某一个过期的key &rarr; 多个request全部都请求DB，导致DB被击穿\n解决方法\n\n预先设置会被多次请求的key，延缓其过期时间\n实时监控，发现某个key被频繁请求，则延缓其过期时间\n使用互斥锁\n在缓存失效时，不会直接去查询DB，而是先使用如SETNX(set if not exist)等会返回“是否操作成功”的操作去设置key-value，只有当上述操作返回成功后，再去访问一次DB，同时缓存该key-value\n\n\n\n缓存雪崩当缓存服务器重启或者大量缓存集中在某一个时间段失效，这样在失效的时候，也会给DB带来很大压力\n解决方法\n\n添加多级缓存，如 nginx + redis\n使用锁/消息队列，但是效率很低，不适用高并发\n提前发现将要过期的key，提前去刷新key\n将缓存失效的时间错开，避免集中在一起失效\n\n\n分布式锁在单机中，可以使用锁实现多线程情况下数据一致性。但是在分布式情况下，多线程运行在不同机器、系统上，为单机加锁没有用。为了控制数据的一致性，需要使用分布式锁。\n要求\n互斥性：在任意时刻，只能有一个客户端有锁\n不会死锁：如果一个客户端有锁时崩溃，没有主动解锁，也能在一定时间后自动放开\n解铃还须系铃人：加锁和解锁必须是同一个主机\n加锁和解锁需要原子性\n\n实现方法基于数据库核心思想\n在数据库中创建一个表，表中包含方法名等字段，并在方法名字段上创建唯一索引，想要执行某个方法，就使用这个方法名向表中插入数据，成功插入则获取锁，执行完成后删除对应的行数据释放锁\n加锁\n想要执行某个方法，就使用这个方法名向表中插入数据。因为我们对method_name做了唯一性约束，这里如果有多个请求同时提交到数据库的话，数据库会保证只有一个操作可以成功，那么我们就可以认为操作成功的那个线程获得了该方法的锁\n解锁\n执行完成后删除对应的行数据释放锁\n问题\n\n因为是基于数据库实现的，数据库的可用性和性能将直接影响分布式锁的可用性及性能，所以，数据库需要双机部署、数据同步、主备切换；\n\n不具备可重入的特性，因为同一个线程在释放锁之前，行数据一直存在，无法再次成功插入数据，所以，需要在表中新增一列，用于记录当前获取到锁的机器和线程信息，在再次获取锁的时候，先查询表中机器和线程信息是否和当前机器和线程相同，若相同则直接获取锁；\n\n没有锁失效机制，会导致死锁。因为有可能出现成功插入数据后，服务器宕机了，对应的数据没有被删除，当服务恢复后一直获取不到锁，所以，需要在表中新增一列，用于记录失效时间，并且需要有定时任务清除这些失效的数据；\n\n不具备阻塞锁特性，获取不到锁直接返回失败，所以需要优化获取逻辑，循环多次去获取。\n\n在实施的过程中会遇到各种不同的问题，为了解决这些问题，实现方式将会越来越复杂；依赖数据库需要一定的资源开销，性能问题需要考虑。\n\n\n基于缓存(性能最高)实现思想\n\n获取锁的时候，使用setnx加锁，并使用expire命令为锁添加一个超时时间，超过该时间则自动释放锁，锁的value值为一个随机生成的UUID，通过此在释放锁的时候进行判断。\n\n获取锁的时候还设置一个获取的超时时间，若超过这个时间则放弃获取锁。\n\n释放锁的时候，通过UUID判断是不是该锁，若是该锁，则执行delete进行锁释放。(为了实现原子性，可以使用LUA脚本实现“检查锁”和“删除锁”)\n\n\n加锁和解锁\nSETNX key val：当且仅当key不存在时，set一个key为val的字符串，返回1；若key存在，则什么都不做，返回0。\nexpire key timeout：为key设置一个超时时间，单位为second，超过这个时间锁会自动释放，避免死锁。\n\n为了实现原子性加锁，可以使用指令set key value nx ex &lt;expireTime&gt;\n\n解锁\ndelete key：删除key\n代码\n# 连接redisredis_client = redis.Redis(...)# 获取一个锁# lock_name：锁定名称# acquire_time: 客户端等待获取锁的时间# time_out: 锁的超时时间def acquire_lock(lock_name, acquire_time=10, time_out=10):    &quot;&quot;&quot;获取一个分布式锁&quot;&quot;&quot;    identifier = str(uuid.uuid4())    end = time.time() + acquire_time    lock = &quot;string:lock:&quot; + lock_name    while time.time() &lt; end:        if redis_client.setnx(lock, identifier):            # 给锁设置超时时间, 防止进程崩溃导致其他进程无法获取锁            redis_client.expire(lock, time_out)            return identifier        elif not redis_client.ttl(lock):            redis_client.expire(lock, time_out)        time.sleep(0.001)    return False# 释放一个锁def release_lock(lock_name, identifier):    &quot;&quot;&quot;通用的锁释放函数&quot;&quot;&quot;    lock = &quot;string:lock:&quot; + lock_name    pip = redis_client.pipeline(True)    while True:        try:            pip.watch(lock)            lock_value = redis_client.get(lock)            if not lock_value:                return True            if lock_value.decode() == identifier:                pip.multi()                pip.delete(lock)                pip.execute()                return True            pip.unwatch()            break        except redis.excetions.WacthcError:            pass    return False\n\n基于zookeeper(可靠性最高)ZooKeeper是一个为分布式应用提供一致性服务的开源组件，它内部是一个分层的文件系统目录树结构，规定同一个目录下只能有一个唯一文件名。\n优缺点\n优点：具备高可用、可重入、阻塞锁特性，可解决失效死锁问题。\n缺点：因为需要频繁的创建和删除节点，性能上不如Redis方式。\n\n","categories":["Redis"],"tags":["redis"]}]